# ğŸš€ RAG API with FastAPI, ChromaDB, Ollama & Kubernetes

An end-to-end **Retrieval-Augmented Generation (RAG)** API built using FastAPI, ChromaDB, and a locally hosted LLM (TinyLlama via Ollama), containerized with Docker and deployed to Kubernetes (Minikube).

This project demonstrates practical AI infrastructure concepts including vector retrieval, model serving, container orchestration, and CI automation.

---

## ğŸ§  Overview

This project implements a lightweight RAG pipeline:

1. User submits a query via HTTP  
2. Relevant context is retrieved from ChromaDB  
3. Context is injected into the prompt  
4. TinyLlama (via Ollama) generates the final response  
5. API returns grounded answer  

The dataset currently includes semantic documentation around:
- Kubernetes fundamentals  
- Nextwork concepts  

---

## ğŸ— Architecture

```bash
User Request
     â†“
FastAPI (/query)
     â†“
ChromaDB (Vector Search)
     â†“
Context Injection
     â†“
Ollama (TinyLlama LLM)
     â†“
Generated Response
```


## Deployment Layer:


---

## ğŸ”§ Tech Stack

- **FastAPI** â€“ REST API framework  
- **Uvicorn** â€“ ASGI server for running FastAPI  
- **ChromaDB** â€“ Vector database for semantic retrieval  
- **Ollama** â€“ Local LLM runtime  
- **TinyLlama** â€“ Lightweight LLM model  
- **Docker** â€“ Containerization  
- **Kubernetes (Minikube)** â€“ Orchestration  
- **GitHub Actions** â€“ CI for semantic document validation  

---

## ğŸ“¦ Features

- Vector-based semantic search using embeddings  
- Context-aware prompt generation  
- Local LLM inference via Ollama  
- Dockerized application  
- Kubernetes deployment (Deployment + Service)  
- NodePort exposure  
- CI pipeline for document validation checks  
- Environment-based configuration  
- Optional mock LLM mode for testing  

---

## ğŸ³ Running Locally (Development Mode)

### Install Dependencies

```bash
pip install -r requirements.txt
```
### Start Ollama
```bash
ollama serve
```
### Ensure Tinyllama is installed

Tinyllama is the LLM that is being used inside Ollama to generate a response for our API

```bash
ollama pull tinyllama
```
### Run embed.py

This initializes the vector DB

```bash
python embed.py
```

### Run API
```bash
uvicorn app:app --reload
```
API is available at http://127.0.0.1:8000

### Example Request

```bash
curl -X POST "http://127.0.0.1:8000/query" \ -G --data-urlencode "q=What is Kubernetes?"
```
## ğŸ³ Docker Usage

## Build Image
```bash
docker build -t rag-app .
```

## Run Container
```bash
docker run -p 8000:8000 rag-app
```
## â˜¸ Kubernetes Deployment (Minikube)

### 1ï¸âƒ£ Point Docker to Minikube
```bash
minikube docker-env | Invoke-Expression
```
### 2ï¸âƒ£ Build Image Inside Cluster
```bash
docker build -t rag-app .
```
### 3ï¸âƒ£ Deploy
```bash
kubectl apply -f k8s/
```
### 4ï¸âƒ£ Access Service
```bash
minikube service rag-app-service --url
```
## ğŸ”„ CI Pipeline

The project includes a GitHub Actions workflow that:

- Validates document structure

- Performs semantic integrity checks

- Builds the Docker image

- Ensures application startup consistency

This ensures RAG document changes maintain expected semantic behavior.

This project was built as part of the NextWork AI DevOps challenge

